"""
    gd(grad, step, x0; maxiter, stop, fun)

Run gradient descent.

# Arguments
- `grad::Function`: Gradient of cost function
- `step::Number`: Step size
- `x0`: Initial guess
- `maxiter::Integer = 100`: Max number of iterations to run
- `stop::Function = (x, iter) -> false`: Function for determining when
    to stop iterating; requires two inputs: the current iterate `x` and the
    current iteration number `iter`
- `fun::Function = (x, iter) -> 0`: Function for computing additional outputs,
    e.g., cost vs iteration; requires two inputs: the current iterate `x` and
    the current iteration number `iter`; called at each iteration

# Return
- `x`: Final iterate, which is presumably close to a true minimizer
- `out::Array{Any,1}`: Output generated by `fun` at each iteration (and before
    the first iteration)
"""
function gd(grad::Function, step::Number, x0;
    maxiter::Integer = 100,
    stop::Function = (x, iter) -> false,
    fun::Function = (x, iter) -> 0
)

    x = copy(x0)
    out = Array{Any}(undef, maxiter + 1)
    out[1] = fun(x, 0)
    iter = 0
    while (iter < maxiter) && !stop(x, iter)
        iter += 1
        x .-= step * grad(x)
        out[iter+1] = fun(x, iter)
    end
    return (x, out[1:iter+1])

end

"""
    lsgd(A, b; x0, step, maxiter, stop, fun)

Solve a least squares problem using gradient descent.

# Arguments
- `A::AbstractMatrix`: Data or system matrix
- `b::AbstractVector`: Labels or data
- `x0::AbstractVector = zeros(eltype(A), size(A, 2))`: Initial guess
- `step::Number = 1 / L`: Step size, where `L` is the exact Lipschitz constant
    `opnorm(A)^2` if `A` is small enough, otherwise it is an upper bound given
    by `opnorm(A, 1) * opnorm(A, Inf)`
- `maxiter::Integer = 100`: Max number of iterations to run
- `stop::Function = (x, iter, cost, grad) -> false`: Function for determining when
    to stop iterating; requires four inputs: the current iterate `x`, the
    current iteration number `iter`, a function `cost` that computes the cost at
    the current iterate, and a function `grad` that computes the gradient of the
    cost function at the current iterate
- `fun::Function = (x, iter, cost, grad) -> 0`: Function for computing additional outputs,
    e.g., cost vs iteration; requires four inputs: the current iterate `x`, the
    current iteration number `iter`, a function `cost` that computes the cost at
    the current iterate, and a function `grad` that computes the gradient of the
    cost function at the current iterate; called at each iteration

# Return
- `x`: Final iterate, which is presumably close to a true minimizer
- `out::Array{Any,1}`: Output generated by `fun` at each iteration (and before
    the first iteration)
"""
function lsgd(A::AbstractMatrix, b::AbstractVector;
    x0::AbstractVector = zeros(eltype(A), size(A, 2)),
    step::Number = 1 / (maximum(size(A)) < 2000 ? opnorm(A)^2 :
                        (opnorm(A, 1) * opnorm(A, Inf))),
    maxiter::Integer = 100,
    stop::Function = (x, iter, cost, grad) -> false,
    fun::Function = (x, iter, cost, grad) -> 0
)

    cost = x -> 0.5norm(A * x - b)^2
    grad = x -> A' * (A * x - b)
    return gd(grad, step, x0,
              maxiter = maxiter,
              stop = (x, iter) -> stop(x, iter, cost, grad),
              fun = (x, iter) -> fun(x, iter, cost, grad))

end

function lsgd(test::Symbol)

    if test == :a1

        A = randn(100, 50)
        b = randn(100)
        xtrue = A \ b
        (xhat, _) = lsgd(A, b, maxiter = 1000)
        @assert xtrue â‰ˆ xhat

    end

end

"""
    gpm(grad, step, proj!, x0; maxiter, stop, fun)

Run gradient projection method.

# Arguments
- `grad::Function`: Gradient of cost function
- `step::Number`: Step size
- `proj!::Function`: Function that projects the input onto the constraint set;
    must be an in-place projection
- `x0`: Initial guess
- `maxiter::Integer = 100`: Max number of iterations to run
- `stop::Function = (x, iter) -> false`: Function for determining when
    to stop iterating; requires two inputs: the current iterate `x` and the
    current iteration number `iter`
- `fun::Function = (x, iter) -> 0`: Function for computing additional outputs,
    e.g., cost vs iteration; requires two inputs: the current iterate `x` and
    the current iteration number `iter`; called at each iteration

# Return
- `x`: Final iterate, which is presumably close to a true minimizer
- `out::Array{Any,1}`: Output generated by `fun` at each iteration (and before
    the first iteration)
"""
function gpm(grad::Function, step::Number, proj!::Function, x0;
    maxiter::Integer = 100,
    stop::Function = (x, iter) -> false,
    fun::Function = (x, iter) -> 0
)

    x = copy(x0)
    out = Array{Any}(undef, maxiter + 1)
    out[1] = fun(x, 0)
    iter = 0
    while (iter < maxiter) && !stop(x, iter)
        iter += 1
        x .-= step * grad(x)
        proj!(x)
        out[iter+1] = fun(x, iter)
    end
    return (x, out[1:iter+1])

end

"""
    nnlsgd(A, b; x0, step, maxiter, stop, fun)

Solve a least squares problem using gradient descent with a nonnegativity
constraint.

# Arguments
- `A::AbstractMatrix`: Data or system matrix
- `b::AbstractVector`: Labels or data
- `x0::AbstractVector = zeros(eltype(A), size(A, 2))`: Initial guess
- `step::Number = 1 / L`: Step size, where `L` is the exact Lipschitz constant
    `opnorm(A)^2` if `A` is small enough, otherwise it is an upper bound given
    by `opnorm(A, 1) * opnorm(A, Inf)`
- `maxiter::Integer = 100`: Max number of iterations to run
- `stop::Function = (x, iter, cost, grad) -> false`: Function for determining when
    to stop iterating; requires four inputs: the current iterate `x`, the
    current iteration number `iter`, a function `cost` that computes the cost at
    the current iterate, and a function `grad` that computes the gradient of the
    cost function at the current iterate
- `fun::Function = (x, iter, cost, grad) -> 0`: Function for computing additional outputs,
    e.g., cost vs iteration; requires four inputs: the current iterate `x`, the
    current iteration number `iter`, a function `cost` that computes the cost at
    the current iterate, and a function `grad` that computes the gradient of the
    cost function at the current iterate; called at each iteration

# Return
- `x`: Final iterate, which is presumably close to a true minimizer
- `out::Array{Any,1}`: Output generated by `fun` at each iteration (and before
    the first iteration)
"""
function nnlsgd(A::AbstractMatrix, b::AbstractVector;
    x0::AbstractVector = zeros(eltype(A), size(A, 2)),
    step::Number = 1 / (maximum(size(A)) < 2000 ? opnorm(A)^2 :
                        (opnorm(A, 1) * opnorm(A, Inf))),
    maxiter::Integer = 100,
    stop::Function = (x, iter, cost, grad) -> false,
    fun::Function = (x, iter, cost, grad) -> 0
)

    cost = x -> 0.5norm(A * x - b)^2
    grad = x -> A' * (A * x - b)
    proj! = x -> replace!(z -> z < 0 ? 0 : z, x)
    return gpm(grad, step, proj!, x0,
               maxiter = maxiter,
               stop = (x, iter) -> stop(x, iter, cost, grad),
               fun = (x, iter) -> fun(x, iter, cost, grad))

end

function nnlsgd(test::Symbol)

    if test == :a1

        A = randn(100, 50)
        b = randn(100)
        (xhat, _) = nnlsgd(A, b)
        @assert count(xhat .< 0) == 0

    end

end
